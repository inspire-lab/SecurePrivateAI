{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "evasion.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/inspire-lab/SecurePrivateAI/blob/master/2_evasion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3W31bZhqItp",
    "colab_type": "text"
   },
   "source": [
    "# Evading SVMs\n",
    "\n",
    "In this section we will be training an SVM (support vector machine) to distinguish between the digits 0 and 1. The data is coming from the MNIST dataset which contains handwritten digits. We will be using `scikit-learn` to train our SVM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPC8dzH8qItq",
    "colab_type": "text"
   },
   "source": [
    "First let's start out by importing a few essentials."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "s97020IAqIts",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPnHl_60qItv",
    "colab_type": "text"
   },
   "source": [
    "Since we are only interested in the 1s and 0s in the data, we will need to pick those out. Since we need to this for both the test and training data, let's write a function for it.\n",
    "\n",
    "This function does a few other things as well. \n",
    "- It normalizes the data, bringing it into the interval [0,1]\n",
    "- It is also only using part of the data to make things a bit faster\n",
    "- It changes the label to -1 and 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d6hPg5bgqItw",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def exract_two_classes( data, labels, classes=(0,1), no_instance=200 ):\n",
    "  \"\"\"\n",
    "  Extract the desired number of instances of given classes from from the `data`.\n",
    "  The function quantizes the data so that all values are in the interval [0,1]. \n",
    "  The returned lables are either -1 or 1. \n",
    "\n",
    "  data: numpy array of instances\n",
    "  labels: numpy array of the labels\n",
    "  classes: tuple of the the two class labels to be extracted\n",
    "  no_instances: number of instances that are returned per class\n",
    "\n",
    "  returns:\n",
    "  x: np array of the instances of both class. The size of the first axis is\n",
    "  no_instance * 2\n",
    "  y: np array containing the labels for the instances in x. the labels have been \n",
    "  transformed so that it is -1 for instances of `classes[0]` and 1 for instances\n",
    "  of `classes[1]`\n",
    "\n",
    "  \"\"\"\n",
    "  pass\n",
    " # return x, y"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNvfSI-4qItz",
    "colab_type": "text"
   },
   "source": [
    "Next we need to load the data and spilt it into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wRUe9zjoqIt0",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# extract two classes using the function we wrote earlier\n",
    "x_train, y_train = exract_two_classes( x_train, y_train, classes=(0,1) )\n",
    "x_test, y_test = exract_two_classes( x_test, y_test, classes=(0,1) )\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1rTmepbqIt4",
    "colab_type": "text"
   },
   "source": [
    "We are going to define a SVM with a RBF (radial basis function) kernel and train it. \n",
    "Once training is done we are going to print the accuracy and show one of the images."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NBetnZElqIt5",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "clf = svm.SVC( )\n",
    "\n",
    "# each innstance needs to be 1 dimensional array. \n",
    "# transform that data:\n",
    "x_train = x_train.reshape( ( x_train.shape[ 0 ], -1 ) )\n",
    "x_test = x_test.reshape( ( x_test.shape[ 0 ], -1 ) )\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "clf.fit( x_train, y_train )\n",
    "print( 'accuracy on test set:', clf.score( x_test, y_test ) )\n",
    "\n",
    "# plot the first instance in the traning set\n",
    "plt.imshow( x_test[ 0 ].reshape( 28, 28 ), cmap=\"gray_r\" )\n",
    "plt.axis( 'off' )\n",
    "plt.show( )"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41DRrXeTtQJy",
    "colab_type": "text"
   },
   "source": [
    "Let's try evading the classifier by hand. Make some changes to sample of your choice. You can either change specific pixels or look into `np.random` for some options to make random changes to the image. Check the output of the classifier for your new sample"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tRlVPYtKtP3A",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# take sample and print some info on it\n",
    "sample = x_test[ 7 ]\n",
    "\n",
    "\n",
    "print( 'output before changes' )\n",
    "plt.imshow( sample.reshape( 28, 28 ), cmap=\"gray_r\" )\n",
    "plt.axis( 'off' )\n",
    "plt.show( )\n",
    "\n",
    "# prediction before changes:\n",
    "print( 'prediction before changes:', clf.predict( [ sample ] ) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###\n",
    "# make your changes here\n",
    "###\n",
    "\n",
    "print( 'output after changes' )\n",
    "plt.imshow( sample.reshape( 28, 28 ), cmap=\"gray_r\" )\n",
    "plt.axis( 'off' )\n",
    "plt.show( )\n",
    "\n",
    "# prediction before changes:\n",
    "print( 'prediction after changes:', clf.predict( [ sample ] ) )\n",
    "\n",
    "\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nr7TEcbqIt9",
    "colab_type": "text"
   },
   "source": [
    "To evade the classifier, we first pick a sample that we want to change. After that we need to retrieve some of the parameters of the SVM which we will need to calculate the gradients."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CJejqaOmqIt-",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# constructing adversarial examples\n",
    "sample = x_test[ 300 ]\n",
    "print( 'class prediction for the test samples:', clf.predict( [ sample ] ) )\n",
    "\n",
    "# Retrieve the internal parameters from the SVM\n",
    "alpha = clf.dual_coef_\n",
    "svs = clf.support_vectors_\n",
    "nsv = svs.shape[ 0 ]\n",
    "b = clf.intercept_\n",
    "\n",
    "plt.imshow( sample.reshape( 28, 28 ), cmap=\"gray_r\" )\n",
    "plt.axis( 'off' )\n",
    "plt.show( )"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4J0c-T80qIuK",
    "colab_type": "text"
   },
   "source": [
    "Now that we have the internal parameters we can calculate the gradients of the SVM and apply the modifications to our selected sample.\n",
    "\n",
    "The strength of the modification is controlled by the `lmbda` parameter.\n",
    "Modifications are applied iteratively controlled by the `iterations` parameter.\n",
    "\n",
    "Make sure that the modified sample is in the interval [0,1]\n",
    "\n",
    "\n",
    "Try to write the code make as little change as possible.\n",
    "\n",
    "Once you have a solution that does not make unnecessary changes. Try different number pairs. 1 and 0 are pretty distinct. Try more and less distinct pairs. How does the behaviour change?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1pMAohmqqIuL",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# the sample we modify\n",
    "mod_sample = sample[ : ]\n",
    "\n",
    "# lambda, strength of the modification\n",
    "lmbd = .01\n",
    "\n",
    "# Compute the gradient and modifiy the sample\n",
    "\n",
    "\n",
    "\n",
    "mod_sample = np.clip( mod_sample, 0., 1. )\n",
    "print( 'class prediction for the original sample:', clf.predict( [sample] ) )\n",
    "print( 'class prediction for the modified sample:', clf.predict( mod_sample ) )\n",
    "print( 'original sample:')\n",
    "plt.imshow( sample.reshape( 28, 28 ), cmap=\"gray_r\" )\n",
    "plt.axis( 'off' )\n",
    "plt.show( )\n",
    "print( 'modified sample:')\n",
    "plt.imshow( mod_sample.reshape( 28, 28 ), cmap=\"gray_r\" )\n",
    "plt.axis( 'off' )\n",
    "plt.show( )\n",
    "\n",
    "\n",
    "print( 'difference between the tow samples:')\n",
    "plt.imshow( np.abs(sample-mod_sample).reshape( 28, 28 ), cmap=\"gray_r\" )\n",
    "plt.axis( 'off' )\n",
    "plt.show( )"
   ],
   "execution_count": 0,
   "outputs": []
  }
 ]
}