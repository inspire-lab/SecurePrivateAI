{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "differantial privacy.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/inspire-lab/SecurePrivateAI/blob/master/7_differantial_privacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3G_Tg5ONqOK",
    "colab_type": "text"
   },
   "source": [
    "# Privacy Preserving Machine Learning\n",
    "\n",
    "First things first. Let's run the package installations. They take quite a while.\n",
    "\n",
    "Change the Runtime of this Notebook to GPU first. Otherwise, it will be pretty slow.\n",
    "\n",
    "To do so go to Runtime -> Change Runtime Type and change it to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install syft==0.2.9 keras==2.2.3 tensorflow_privacy==0.2.2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EeT48vu3PmAa",
    "colab_type": "text"
   },
   "source": [
    "## Differential Privacy\n",
    "\n",
    "Below we will train a model perform malware detection. Image classification on the MNIST data. will train it using Differentially Private SGD optimizer.\n",
    "\n",
    "How does the privacy budget `epsilon` change when you tweak the parameters of the optimizer? How does it influence accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLVBkf5QhKDy",
    "colab_type": "text"
   },
   "source": [
    "## Parameters for the optimizer\n",
    "\n",
    "All the optimizers share some privacy-specific parameters that need to\n",
    "be tuned in addition to any existing hyperparameter. There are currently four:\n",
    "\n",
    "* `learning_rate` (float): The learning rate of the SGD training algorithm. The\n",
    "  higher the learning rate, the more each update matters. If the updates are noisy\n",
    "  (such as when the additive noise is large compared to the clipping\n",
    "  threshold), the learning rate must be kept low for the training procedure to converge.\n",
    "* `num_microbatches` (int): The input data for each step (i.e., batch) of your\n",
    "  original training algorithm is split into this many micro-batches. Generally,\n",
    "  increasing this will improve your utility but slow down your training in terms\n",
    "  of wall-clock time. The total number of examples consumed in one global step\n",
    "  remains the same. This number should evenly divide your input batch size.\n",
    "* `l2_norm_clip` (float): The cumulative gradient across all network parameters\n",
    "  from each micro-batch will be clipped so that its L2 norm is at most this\n",
    "  value. You should set this to something close to some percentile of what\n",
    "  you expect the gradient from each micro-batch to be. In previous experiments,\n",
    "  we've found numbers from 0.5 to 1.0 to work reasonably well.\n",
    "* `noise_multiplier` (float): This governs the amount of noise added during\n",
    "  training. Generally, more noise results in better privacy and lower utility.\n",
    "  This generally has to be at least 0.3 to obtain rigorous privacy guarantees,\n",
    "  but smaller values may still be acceptable for practical purposes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gRhkF2sojr_y",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "(mnist_x_train, mnist_y_train), (mnist_x_test, mnist_y_test) = mnist.load_data()\n",
    "\n",
    "mnist_x_train = mnist_x_train.astype( np.float32 ) / 255\n",
    "mnist_x_test = mnist_x_test.astype( np.float32 ) / 255\n",
    "\n",
    "mnist_x_train = mnist_x_train.reshape( -1, 28, 28, 1)\n",
    "mnist_x_test = mnist_x_test.reshape( -1, 28, 28, 1)\n",
    "\n",
    "\n",
    "mnist_y_train = keras.utils.to_categorical( mnist_y_train )\n",
    "mnist_y_test = keras.utils.to_categorical( mnist_y_test )\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 250\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dv_M25D9HYeS",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# training parameters\n",
    "l2_norm_clip = \n",
    "noise_multiplier = \n",
    "num_microbatches = \n",
    "learning_rate = \n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add( tf.keras.layers.Conv2D( 32, kernel_size=(3, 3), activation='relu', input_shape=mnist_x_train.shape[ 1: ]  ) )\n",
    "model.add( tf.keras.layers.MaxPooling2D( pool_size=(2, 2) ) )\n",
    "model.add( tf.keras.layers.Conv2D( 64, kernel_size=(3, 3), activation='relu' ) )\n",
    "model.add( tf.keras.layers.Flatten() )\n",
    "model.add( tf.keras.layers.Dense( 128, activation='relu' ) )\n",
    "model.add( tf.keras.layers.Dense( 10, activation='softmax' ) )\n",
    "\n",
    "\n",
    "optimizer = DPGradientDescentGaussianOptimizer(\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    num_microbatches=num_microbatches,\n",
    "    learning_rate=0.01)\n",
    "\n",
    "loss = tf.keras.losses.CategoricalCrossentropy( from_logits=True, reduction=tf.losses.Reduction.NONE )\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "model.fit(mnist_x_train, mnist_y_train,\n",
    "          epochs=EPOCHS,\n",
    "          batch_size=BATCH_SIZE)\n",
    "\n",
    "print( 'test acc:', model.evaluate( mnist_x_test, mnist_y_test, batch_size=250 )[ 1 ] )\n",
    "\n",
    "eps = compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=60000, batch_size=250, noise_multiplier=noise_multiplier, epochs=15, delta=1e-5)\n",
    "print( 'epsilon: ', eps )\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlR0Dnw8HeyJ",
    "colab_type": "text"
   },
   "source": [
    "# Pate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWbpCjGGHgKE",
    "colab_type": "text"
   },
   "source": [
    "First we need to split up the data and train the teachers. For simplicity, we will work with 3 teachers and a small amount of data.\n",
    "\n",
    "\n",
    "We are going to split the data into 4 partitions of 500 instances each. 3 partitions for the teachers and one for the students.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_FxG_dwj_Rb",
    "colab_type": "text"
   },
   "source": [
    "## Review Pate\n",
    "\n",
    "![alt text](http://www.cleverhans.io/assets/pate-aggregation.png)\n",
    "\n",
    "![alt text](http://www.cleverhans.io/assets/pate-full.png)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OaZNmdzeICtE",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "# number of instances each party has access to \n",
    "n_instances = 500 \n",
    "#n number of teachers\n",
    "n_teachers = 3\n",
    "\n",
    "# load data and transform it\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype( float ) / 255.\n",
    "x_test = x_test.astype( float ) / 255.\n",
    "\n",
    "x_train = x_train.reshape( -1, 28, 28, 1)\n",
    "x_test = x_test.reshape( -1, 28, 28, 1)\n",
    "\n",
    "y_train = keras.utils.to_categorical( y_train )\n",
    "y_test = keras.utils.to_categorical( y_test )\n",
    "\n",
    "# shuffle data\n",
    "idx = np.arange( len( x_train ) )\n",
    "np.random.shuffle( idx )\n",
    "x_train = x_train[ idx ]\n",
    "y_train = y_train[ idx ]\n",
    "\n",
    "# gather the teacher data\n",
    "teacher_data_x = \n",
    "teacher_data_y = \n",
    "\n",
    "# gather the student data\n",
    "student_data_x = \n",
    "student_data_y = \n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "N45SSQDAMwaK",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# train the teacher models\n",
    "def get_model():\n",
    "  model = keras.models.Sequential()\n",
    "  model.add( keras.layers.Conv2D( 32, 3, 2, activation='relu', input_shape=x_train.shape[ 1: ] ) )\n",
    "  model.add( keras.layers.MaxPooling2D( ) )\n",
    "  model.add( keras.layers.Conv2D( 16, 3, 2, activation='relu' ) )\n",
    "  model.add( keras.layers.Flatten() )\n",
    "  model.add( keras.layers.Dense(32, activation='relu') )\n",
    "  model.add( keras.layers.Dense(10, activation='softmax') )\n",
    "\n",
    "  model.compile( optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'] )\n",
    "\n",
    "  return model\n",
    "\n",
    "# list of teacher models\n",
    "teacher_models = \n",
    "\n",
    "# train teacher models\n",
    "\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCkzvxlER6EQ",
    "colab_type": "text"
   },
   "source": [
    "## Train the student model\n",
    "\n",
    "To train the student model we need to label the students training data using the teacher models. We'll use a majority voting with added noises to determine the label."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Q-GHRrstSuAi",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# label the data\n",
    "labels = \n",
    "# preform the voting\n",
    "votes = \n",
    "\n",
    "# transform the noisy votes into the student labels\n",
    "student_data_y = \n",
    "\n",
    "# train model\n",
    "student_model = get_model()\n",
    "print( 'training student model' )\n",
    "student_model.fit( x, y, epochs=16, verbose=0 )\n",
    "print( 'test accuracy:', student_model.evaluate( student_data_x, student_data_y, verbose=0 )[ 1 ] )\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# privacy analysis\n",
    "from syft.frameworks.torch.dp import pate\n",
    "\n",
    "\n",
    "teacher_preds = np.argmax( np.array( labels ), axis=2 )\n",
    "print( teacher_preds.shape )\n",
    "\n",
    "data_dep_eps, data_indep_eps = pate.perform_analysis( teacher_preds=teacher_preds,\n",
    "                                                      indices=np.argmax( votes, axis=1 ),\n",
    "                                                      noise_eps=0.2,\n",
    "                                                      delta=1/1500\n",
    "                                                     )\n",
    "\n",
    "print(data_dep_eps, data_indep_eps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What's more. Try by yourself with PyTorch\n",
    "\n",
    "https://opacus.ai/tutorials/building_image_classifier\n",
    "\n",
    "[Opacus](https://github.com/pytorch/opacus) is a library that enables training PyTorch models with differential privacy. It supports training with minimal code changes required on the client, has little impact on training performance and allows the client to online track the privacy budget expended at any given moment.\n",
    "\n",
    "Just click\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/pytorch/opacus/blob/main/tutorials/building_image_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "It comes from\n",
    "https://github.com/pytorch/opacus/blob/main/tutorials/building_image_classifier.ipynb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And another demo\n",
    "\n",
    "https://github.com/erinqhu/differential-privacy-PATE/blob/master/PATE_analysis.ipynb\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/erinqhu/differential-privacy-PATE/blob/master/PATE_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ]
}