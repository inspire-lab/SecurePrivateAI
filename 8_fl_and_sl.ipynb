{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "fl and sl.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/inspire-lab/SecurePrivateAI/blob/master/8_fl_and_sl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3G_Tg5ONqOK",
    "colab_type": "text"
   },
   "source": [
    "# Privacy Preserving Machine Learning\n",
    "\n",
    "First things first. Let's run the package installations. They take quite a while.\n",
    "\n",
    "Add the end of the installation you need to hit the restart button. \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rImgnfa-h99n",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "!pip install tensorflow_federated pysyft==0.2.5"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3K1fejzQOHlv",
    "colab_type": "text"
   },
   "source": [
    "Next we'll get our usual boilerplate code out of the way. Data loading, splitting, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Orh1mHXqOh0u",
    "colab_type": "text"
   },
   "source": [
    "Load our data set and split it into test and training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "n_instances = 32 # the number of instance each party has\n",
    "n_parties = 10 # number of parties\n",
    "\n",
    "# load data and transform it\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = np.concatenate( [ x_train[ y_train == 0 ][ :200 ], x_train[ y_train == 1 ][ :200 ] ] ).reshape( (400,-1) )\n",
    "x_test = np.concatenate( [ x_test[ y_test == 0 ][ :200 ], x_test[ y_test == 1 ][ :200 ] ] ).reshape( (400,-1) )\n",
    "x_train = x_train.astype( float ) / 255.\n",
    "x_test = x_test.astype( float ) / 255.\n",
    "\n",
    "print( 'training data: ', x_train.shape )\n",
    "print( 'test data: ', x_test.shape )\n",
    "\n",
    "# labels\n",
    "y_train = np.concatenate( [ np.zeros( 200 ), np.ones( 200 ) ] )\n",
    "y_test = np.concatenate( [ np.zeros( 200 ), np.ones( 200 ) ] )\n",
    "\n",
    "idx = np.arange( len( x_train ) )\n",
    "np.random.shuffle( idx )\n",
    "x_train = x_train[ idx ]\n",
    "y_train = y_train[ idx ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aK6BBE6YPxIr",
    "colab_type": "text"
   },
   "source": [
    "# Federated Learning\n",
    "\n",
    "Federated Learning is the idea of sending the model to multiple clients. The clients train on their own data and send the updates back to the server. On the server the individual updates are aggregated.\n",
    "\n",
    "![The pipeline](https://github.com/inspire-lab/SecurePrivateAI/raw/main/images/FL.png)\n",
    "\n",
    "We will pretend that data is sent between the parties but for simplicity the data never leaves this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMYqzecI3v33",
    "colab_type": "text"
   },
   "source": [
    "We are using a simple perceptron as our learning algorithm. We will assume we have 10 parties and give them all small sub set of the data. The goal of the perceptron is to distinguish 0s and 1s from the MNIST data set.\n",
    "\n",
    "The perceptron algorithm is simple for an input vector $x$,  the weight vector $w$ and the intercept $b$\n",
    "the output $y$ is calculated as:\n",
    "\n",
    "$$ y = \\sum_{i=1}^{n} x_i w_i $$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OaZNmdzeICtE",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# first practice to do\n",
    "# gather the data for each party\n",
    "data_x = [  ]\n",
    "data_y = [  ]\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FnccveiWHDe",
    "colab_type": "text"
   },
   "source": [
    "Now that we have our data let's tackle the learning problem in a none federated setting for a single party.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "y2w52qX4VU6_",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# initialize the weights of our perceptron\n",
    "weights =\n",
    " \n",
    "# learning rate\n",
    "lr = 0.1\n",
    "# batch size\n",
    "bs = 16\n",
    "\n",
    "# the training function\n",
    "def train( weights, x, y ):\n",
    "  # train for a number of epochs\n",
    "  for e in range( 10 ):\n",
    "    acc = [] \n",
    "    print( 'epoch', e )\n",
    "    # the actual sgd\n",
    "    for i in range( len( x ) // bs ):\n",
    "      batch_x = x[ i * bs : (i+1) * bs ]\n",
    "      batch_y = y[ i * bs : (i+1) * bs ]\n",
    "      \n",
    "      # forward step\n",
    "      f =\n",
    "      \n",
    "      # apply sigmoid\n",
    "      f = \n",
    "      \n",
    "      # calculate the error\n",
    "      error = \n",
    "      # calculate gradient\n",
    "      g = \n",
    "      \n",
    "      # weight update \n",
    "      weights = \n",
    "      # calculate acc\n",
    "      preds = \n",
    "      batch_acc = \n",
    "      acc.append( batch_acc )\n",
    "    print( 'acc: ', sum( acc ) / len( acc ) )\n",
    "    \n",
    "  # test data\n",
    "  prediction =\n",
    "  print( 'test acc:',\n",
    "\n",
    "train( weights, data_x[ 0 ], data_y[ 1 ] )\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4uQx0UPaBIn",
    "colab_type": "text"
   },
   "source": [
    "Now that we have solved it for one party lets modify the code to send the gradient updates to a central party that aggregates it and updates the weights."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Dq37l0iMZ0SJ",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# the local weights of every party \n",
    "models = \n",
    "\n",
    "def get_gradient( weights, x, y ):\n",
    "  \"\"\"\n",
    "  returns the gradients wrt. loss for the training samples\n",
    "  \"\"\"\n",
    "  grads = []\n",
    "  for i in range( len( x ) // bs ):\n",
    "    batch_x = x_train[ i * bs : (i+1) * bs ]\n",
    "    batch_y = y_train[ i * bs : (i+1) * bs ]\n",
    "    # forward step\n",
    "    f = \n",
    "    # sigmoid\n",
    "    f = \n",
    "    # loss\n",
    "    error = \n",
    "    # gradient\n",
    "    g =\n",
    "    # save the gradients from this batch \n",
    "    grads.append( g )  \n",
    "  \n",
    "  # return the average of the gradients\n",
    "  return \n",
    "\n",
    "# the servers weights\n",
    "w =\n",
    "# do 10 epochs\n",
    "for i in range( 10 ):\n",
    "  # get all gradients from the parties\n",
    "  gradients = [ get_gradient( models[ i ], data_x[ i ], data_y[ i ] ) for i in range( n_parties ) ]\n",
    "\n",
    "  # average gradients\n",
    "  gradients =\n",
    "  # do weight updates\n",
    "  w -=\n",
    "  # test data\n",
    "  prediction =\n",
    "  print( 'test acc:',\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITw3Ial1cVby",
    "colab_type": "text"
   },
   "source": [
    "The problem with this solution is that their aggregator can see the weights of the individual parties. This is a potential information leak.\n",
    "\n",
    "A solution to this problem is applying a mask to the data. But since we still want to have usable gradients we need a way to remove the mask again. We assume that the clients have formed pairs and securely shared a mask between them. Each client has been assigned a unique id as well. The masking process is as follows:\n",
    "\n",
    "\n",
    "1.  The client with the smaller id adds the mask to the gradient\n",
    "2.  The client with the larger id subtracts the mask from the gradient\n",
    "3. When gradients are aggregated by the server the masks will cancel each other out and only the sum of the gradients remains\n",
    "\n",
    "For simplicity, we are not making the protocol resistant to dropouts. This would require forming pairs between all clients and additional masking to ensure the privacy of the data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oLG_KVuVtlYj",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# the local weights of every party \n",
    "models =\n",
    "\n",
    "# shared masks. 0 and 5, 1 and 6, etc share a mask\n",
    "masks = \n",
    "\n",
    "\n",
    "\n",
    "# learning rate\n",
    "lr = 0.1\n",
    "# batch size\n",
    "bs = 16\n",
    "\n",
    "def get_gradient_masked( weights, x, y, id ):\n",
    "  grads = []\n",
    "  for i in range( len( x ) // bs ):\n",
    "    batch_x = x_train[ i * bs : (i+1) * bs ]\n",
    "    batch_y = y_train[ i * bs : (i+1) * bs ]\n",
    "    # forward step\n",
    "    f = \n",
    "    # sigmoid\n",
    "    f = \n",
    "    # loss\n",
    "    error = \n",
    "    # gradient\n",
    "    g =\n",
    "    grads.append( g )  \n",
    "\n",
    "  g = \n",
    "  # add the mask\n",
    "\n",
    "  return g\n",
    "\n",
    "w \n",
    "for i in range( 10 ):\n",
    "  # get all gradients\n",
    "  gradients = [ get_gradient_masked( models[ i ], data_x[ i ], data_y[ i ], i ) for i in range( n_parties ) ]\n",
    "\n",
    "  # average gradients\n",
    "  gradients = \n",
    "  # do weight updates\n",
    "  w = \n",
    "  # test data\n",
    "  prediction = \n",
    "  print( 'test acc:', "
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ciVHvt2itsmb",
    "colab_type": "text"
   },
   "source": [
    "## Federated Learning with TensorFlow\n",
    "\n",
    "Out of the pieces that we have assembled above we can build a federated learning algorithm. An alternative is to use the implementation that is provided by TensorFlow. Below is code that trains a model on distributed data using secure aggregation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4xxI4Gi_h94u",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = np.concatenate( [ x_train[ y_train == 0 ], x_train[ y_train == 1 ] ] )\n",
    "x_test = np.concatenate( [ x_test[ y_test == 0 ], x_test[ y_test == 1 ] ] )\n",
    "x_train = x_train.astype( float ) / 255.\n",
    "x_test = x_test.astype( float ) / 255.\n",
    "\n",
    "x_train = x_train.reshape( ( x_train.shape[ 0 ], -1  ) )\n",
    "x_test = x_test.reshape( ( x_test.shape[ 0 ], -1  ) )\n",
    "print( x_train.shape )\n",
    "\n",
    "# labels\n",
    "y_train = np.concatenate( [ np.zeros( np.sum( y_train == 0 ) ), np.ones( np.sum( y_train == 1  ) ) ]  )\n",
    "y_test = np.concatenate( [ np.zeros( np.sum( y_test == 0 ) ), np.ones( np.sum( y_test == 1 ) ) ]  ) \n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8eVxRHn2tzcb",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "# parameters\n",
    "NO_CLIENTS = 3 # number of clients\n",
    "TOTAL_SAMPLES = x_train.shape[ 0 ]\n",
    "NO_CLIENT_SAMPLES = TOTAL_SAMPLES // NO_CLIENTS # number of samples per client\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "# shuffle data\n",
    "idx = np.arange( TOTAL_SAMPLES )\n",
    "np.random.shuffle( idx )\n",
    "x_train = x_train[ idx ]\n",
    "y_train = y_train[ idx ]\n",
    "\n",
    "data = []\n",
    "# split into clients\n",
    "for i in range( NO_CLIENTS ):\n",
    "  x = x_train[ i * NO_CLIENT_SAMPLES : ( i + 1 ) * NO_CLIENT_SAMPLES  ]\n",
    "  print( x.shape )\n",
    "  y = y_train[ i * NO_CLIENT_SAMPLES : ( i + 1 ) * NO_CLIENT_SAMPLES ].reshape( [-1,1] ) \n",
    "  print( y.shape )\n",
    "  ds = tf.data.Dataset.from_tensor_slices( (x.astype( np.float ) , y.astype( np.float ) ) )\n",
    "  ds = ds.repeat( EPOCHS ).shuffle( 200 ).batch( BATCH_SIZE )\n",
    "\n",
    "  print( ds )\n",
    "  data.append( ds )\n",
    "\n",
    "\n",
    "# define a function that builds our model\n",
    "def build_model():\n",
    "  model = tf.keras.models.Sequential()\n",
    "\n",
    "  model.add( tf.keras.layers.Dense( 64, activation='relu', input_shape=( x_train.shape[ 1: ] ) ) )\n",
    "  model.add( tf.keras.layers.Dense( 2, activation='softmax' ) )\n",
    "\n",
    "  return model\n",
    "\n",
    "def model_function():\n",
    "  # we need a dummy batch to build the federated model\n",
    "  # From the docs:\n",
    "  # A nested structure of values that are convertible to batched tensors\n",
    "  # with the same shapes and types as expected by forward_pass(). \n",
    "  # The values of the tensors are not important and can be filled with any \n",
    "  # reasonable input value.\n",
    "  dummy_batch = collections.OrderedDict( [ \n",
    "      ('x', np.ones( ( BATCH_SIZE, x_train.shape[ 1 ] ) ) ),\n",
    "      ('y', np.ones( ( BATCH_SIZE, 1) ) ) ] )\n",
    "\n",
    "  # get the compiled keras model\n",
    "  model = build_model()\n",
    "  # use tensorflow function to create a federated learning model\n",
    "  return tff.learning.from_keras_model( model, loss=tf.keras.losses.SparseCategoricalCrossentropy(),  dummy_batch=dummy_batch, metrics=[tf.keras.metrics.SparseCategoricalAccuracy() ] )\n",
    "\n",
    "\n",
    "# use tensorflow to create the averaging algorithm\n",
    "algorithm = tff.learning.build_federated_averaging_process( model_function, client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02 ) ) \n",
    "\n",
    "# initialize the learning algorithm and get the initial state\n",
    "state = algorithm.initialize()\n",
    "\n",
    "# run the training steps\n",
    "for e in range( EPOCHS ):\n",
    "  state, metrics = algorithm.next( state, data )\n",
    "  print( 'epoch' , e , metrics )\n",
    "\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQZYyneKMQs5",
    "colab_type": "text"
   },
   "source": [
    "# Split Learning and Pysyft\n",
    "\n",
    "Split learning considers from another perspective\n",
    "\n",
    "![pipeline](https://github.com/inspire-lab/SecurePrivateAI/raw/main/images/SL.png)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hij96mrczy5K",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import torch\n",
    "import syft as sy\n",
    "\n",
    "# allow pysyft to work its magic on torch tensors\n",
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "# create a virtual worker. in an actual setting this would be on a different machine\n",
    "client = sy.VirtualWorker( hook, id='client' )\n",
    "\n",
    "# define a tensor and send it to the client\n",
    "x = torch.tensor([1,2,3,4,5])\n",
    "# this leaves us with a pointer to the tensor\n",
    "x_pointer = x.send( client )\n",
    "\n",
    "# check out some meta data\n",
    "print( x_pointer )\n",
    "print( client._objects )\n",
    "\n",
    "# we can use this pointers like normal tensors\n",
    "result = x_pointer + x_pointer\n",
    "print( result )\n",
    "\n",
    "# if we want the result we can call get() to send the tensor back to us\n",
    "result_local = result.get()\n",
    "# once we call get() it removes the tensor from the other side and our pointer\n",
    "# becomes invalid\n",
    "print( result_local )\n",
    "print( client._objects )\n",
    "# print( result )"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Zur5oik9MS2c",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "import syft as sy\n",
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "# Data preprocessing\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "train_set = datasets.MNIST('mnist', download=True, train=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Define our model segments\n",
    "\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 640]\n",
    "output_size = 10\n",
    "\n",
    "models = [\n",
    "    nn.Sequential(\n",
    "                nn.Linear(input_size, hidden_sizes[0]),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                nn.ReLU(),\n",
    "    ),\n",
    "    nn.Sequential(\n",
    "                nn.Linear(hidden_sizes[1], output_size),\n",
    "                nn.LogSoftmax(dim=1)\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create optimisers for each segment and link to their segment\n",
    "optimizers = [\n",
    "    optim.SGD(model.parameters(), lr=0.03,)\n",
    "    for model in models\n",
    "]\n",
    "\n",
    "# create some workers\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "workers = alice, bob\n",
    "\n",
    "# Send Model Segments to starting locations\n",
    "model_locations = [alice, bob]\n",
    "\n",
    "for model, location in zip(models, model_locations):\n",
    "    model.send(location)\n",
    "\n",
    "def train(x, target, models, optimizers):\n",
    "    # Training Logic\n",
    "\n",
    "    #1) erase previous gradients (if they exist)\n",
    "    for opt in optimizers:\n",
    "        opt.zero_grad()\n",
    "\n",
    "    #2) make a prediction\n",
    "    a = models[0](x)\n",
    "\n",
    "    #3) break the computation graph link, and send the activation signal to the next model\n",
    "    remote_a = a.move(models[1].location, requires_grad=True)\n",
    "\n",
    "    #4) make prediction on next model using received signal\n",
    "    pred = models[1](remote_a)\n",
    "\n",
    "    #5) calculate how much we missed\n",
    "    criterion = nn.NLLLoss()\n",
    "    loss = criterion(pred, target)\n",
    "\n",
    "    #6) figure out which weights caused us to miss\n",
    "    loss.backward()\n",
    "\n",
    "    # 7) send gradient of the received activation signal to the model behind\n",
    "    # grad_a = remote_a.grad.copy().move(models[0].location)\n",
    "\n",
    "    # 8) backpropagate on bottom model given this gradient\n",
    "    # a.backward(grad_a)\n",
    "\n",
    "    #9) change the weights\n",
    "    for opt in optimizers:\n",
    "        opt.step()\n",
    "\n",
    "    #10) print our progress\n",
    "    return loss.detach().get()\n",
    "\n",
    "for i in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.send(alice)\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        labels = labels.send(bob)\n",
    "        \n",
    "        loss = train(images, labels, models, optimizers)\n",
    "        running_loss += loss\n",
    "\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(i, running_loss/len(train_loader)))\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "More references:\n",
    "\n",
    "https://learnopencv.com/federated-learning-using-pytorch-and-pysyft/\n",
    "\n",
    "https://github.com/OpenMined/PySyft/tree/syft_0.2.x/examples/tutorials/advanced/split_neural_network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ]
}