{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/inspire-lab/SecurePrivateAI/blob/master/2_attack_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d1IIenRzV7Gv"
   },
   "source": [
    "# Attacking a CNN\n",
    "\n",
    "The CNN is vunerable to adversarial examples as ![adv example](https://www.tensorflow.org/tutorials/generative/images/adversarial_example.png)\n",
    "\n",
    "In this exercise we will train a CNN to distinguish between instances of handwritten `0` and instances of handwritten `1`. We will be using `PyTorch` to do this.  \n",
    "\n",
    "Once we have a trained classifier, we will create adversarial examples from scratch and using `ART`\n",
    "\n",
    "This is adopted from https://github.com/Trusted-AI/adversarial-robustness-toolbox/blob/main/examples/get_started_pytorch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T14:11:08.979581Z",
     "start_time": "2021-09-15T14:11:08.925577Z"
    }
   },
   "outputs": [],
   "source": [
    "# some configurations for jupyter notebook\n",
    "%config Completer.use_jedi = False\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T14:11:11.174848Z",
     "start_time": "2021-09-15T14:11:09.758815Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "nhoEjgYmWJ0E",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install adversarial-robustness-toolbox torch torchvision numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T14:11:13.526513Z",
     "start_time": "2021-09-15T14:11:12.581882Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "iIH4d-w4V7G7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIL2ziyzV7G_"
   },
   "source": [
    "The MNIST dataset contains data for all of the digits, \n",
    "\n",
    "We need to normalize the data. But here, we use the API from `ART`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y3nYU03lV7HD"
   },
   "source": [
    "Load the actual data. It will load the data as numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T14:12:05.184519Z",
     "start_time": "2021-09-15T14:12:04.164149Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "CMKzVNfRV7HA"
   },
   "outputs": [],
   "source": [
    "from art.attacks.evasion import FastGradientMethod\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "from art.utils import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T14:12:10.034551Z",
     "start_time": "2021-09-15T14:12:09.061808Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Load the MNIST dataset\n",
    "\n",
    "(x_train, y_train), (x_test, y_test), min_pixel_value, max_pixel_value = load_mnist()\n",
    "\n",
    "# Step 1a: Swap axes to PyTorch's NCHW format\n",
    "\n",
    "x_train = np.transpose(x_train, (0, 3, 1, 2)).astype(np.float32)\n",
    "x_test = np.transpose(x_test, (0, 3, 1, 2)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(x_train))\n",
    "print(x_train.shape, x_test.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d64xNkKdV7HX"
   },
   "source": [
    "We are using a very simple CNN. This network can be used to distinguish between all 10 classes with very high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T14:12:12.335483Z",
     "start_time": "2021-09-15T14:12:12.292986Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "GMjW64ADV7HY"
   },
   "outputs": [],
   "source": [
    "# define the classifier\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv_1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=5, stride=1)\n",
    "        self.conv_2 = nn.Conv2d(in_channels=4, out_channels=10, kernel_size=5, stride=1)\n",
    "        self.fc_1 = nn.Linear(in_features=4 * 4 * 10, out_features=100)\n",
    "        self.fc_2 = nn.Linear(in_features=100, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv_1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv_2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4 * 4 * 10)\n",
    "        x = F.relu(self.fc_1(x))\n",
    "        x = self.fc_2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we initialize a model and train with the cross-entropy loss.\n",
    "\n",
    "To simplify the training code, we use the wrapper `PyTorchClassifier` from `ART` to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T14:12:29.133135Z",
     "start_time": "2021-09-15T14:12:14.293045Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Create the model\n",
    "\n",
    "model = Net()\n",
    "\n",
    "# Step 2a: Define the loss function and the optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Step 3: Create the ART classifier\n",
    "\n",
    "classifier = PyTorchClassifier(\n",
    "    model=model,\n",
    "    clip_values=(min_pixel_value, max_pixel_value),\n",
    "    loss=criterion,\n",
    "    optimizer=optimizer,\n",
    "    input_shape=(1, 28, 28),\n",
    "    nb_classes=10,\n",
    ")\n",
    "\n",
    "# Step 4: Train the ART classifier\n",
    "\n",
    "classifier.fit(x_train, y_train, batch_size=64, nb_epochs=3)\n",
    "\n",
    "# Step 5: Evaluate the ART classifier on benign test examples\n",
    "\n",
    "predictions = classifier.predict(x_test)\n",
    "accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
    "print(\"Accuracy on benign test examples: {}%\".format(accuracy * 100))\n",
    "\n",
    "# you should see an accuracy > 95%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bn0dNIotIQ_n"
   },
   "source": [
    "Let's get to the actual attack magic. First we are picking a sample that we want to perturbate. After that we will be implementing our own FGSM attack. \n",
    "\n",
    "The attack is fairly simple. It consists of the following steps: \n",
    "\n",
    "1.   Compute the loss of the original sample\n",
    "2.   Calculate the gradient of the loss w.r.t the input \n",
    "3.   Take the sign of the gradient and add a fraction episilon to the input, namely $x + \\epsilon sign(\\nabla_x J(x, y))$\n",
    "\n",
    "Epsilon controlls the strenght of the pertubation.\n",
    "\n",
    "First, we select a sample to visualize it and output the model's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T14:13:52.618948Z",
     "start_time": "2021-09-15T14:13:52.453189Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# chose a sample to pertubate\n",
    "sample_ind = 3 # chosen by totaly random dice roll,  index=3 ->  the data is `0`\n",
    "\n",
    "# picking a test sample\n",
    "sample = x_test[ sample_ind, : ]\n",
    "\n",
    "print( sample.shape )\n",
    "\n",
    "# plot the first instance in the traning set\n",
    "plt.imshow( sample.reshape( 28, 28 ), cmap=\"gray_r\" )\n",
    "plt.axis( 'off' )\n",
    "plt.show( )\n",
    "\n",
    "\n",
    "pred_prob = F.softmax(model( torch.FloatTensor(sample.reshape( (1, sample.shape[ 0 ], sample.shape[ 1 ], sample.shape[ 2 ]) ) ) ), dim=1)\n",
    "\n",
    "logits = classifier.predict( sample.reshape( (1, sample.shape[ 0 ], sample.shape[ 1 ], sample.shape[ 2 ]) ) )\n",
    "\n",
    "print( 'output for the test samples:\\n', logits )\n",
    "print( 'class prediction for the test samples:\\n', pred_prob.detach() )\n",
    "print( 'predicted as', np.argmax( logits , axis=1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `ART` loads data as numpy array, we create variables as PyTorch Tensor for convenience\n",
    "\n",
    "And the labels of `y_train` and `y_test` are one-hot vectors. We need to convert it to the category label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T14:14:02.986788Z",
     "start_time": "2021-09-15T14:14:02.940576Z"
    }
   },
   "outputs": [],
   "source": [
    "eps = 1. # allowed maximum modification\n",
    "t_sample = torch.FloatTensor(sample.reshape( (1, sample.shape[ 0 ], sample.shape[ 1 ], sample.shape[ 2 ]) ) )\n",
    "one_hot_y = torch.LongTensor( y_test[ sample_ind, : ].reshape( ( 1, -1 ) ) )\n",
    "t_y = torch.argmax(one_hot_y, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct adversarial examples from scratch. You can use the code above as reference.\n",
    "\n",
    "Eq: $x + \\epsilon sign(\\nabla_x J(x, y))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T14:14:37.298405Z",
     "start_time": "2021-09-15T14:14:37.238432Z"
    }
   },
   "outputs": [],
   "source": [
    "# constructing adversarial examples\n",
    "######################\n",
    "# fill in the blanks #\n",
    "######################\n",
    "\n",
    "# compute logits using the PyTorch Tensor\n",
    "\n",
    "logits = ??? t_sample ???\n",
    "\n",
    "# compute the cross entropy loss of our original sample\n",
    "\n",
    "loss = \n",
    "\n",
    "# get the gradient w.r.t to the input. \n",
    "# Here it may show an error, if you exactly follow the tutorial\n",
    "\n",
    "grads = torch.autograd.grad(  )\n",
    "print(grads.shape)\n",
    "\n",
    "# You may see an error\n",
    "# The error should be `RuntimeError: One of the differentiated Tensors does not require grad`\n",
    "# What does it mean? and how to solve it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's caused by the mechanism of PyTorch.\n",
    "\n",
    "By default, only model's parameters will compute/require gradients.\n",
    "\n",
    "Now we need to firstly let the input data require gradients.\n",
    "\n",
    "The adversarial example is: $x + \\epsilon sign(\\nabla_x J(x, y))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T14:16:22.392365Z",
     "start_time": "2021-09-15T14:16:22.250816Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "RovKr9sKIQgh"
   },
   "outputs": [],
   "source": [
    "# constructing adversarial examples\n",
    "######################\n",
    "# fill in the blanks #\n",
    "######################\n",
    "\n",
    "# Set the data require gradients\n",
    "\n",
    "\n",
    "\n",
    "# compute logits\n",
    "\n",
    "logits = ??? t_sample ???\n",
    "\n",
    "# compute the loss of our original sample\n",
    "\n",
    "loss =  \n",
    "\n",
    "# get the gradient wrt to the input.\n",
    "\n",
    "grads = torch.autograd.grad( )\n",
    "\n",
    "# use torch.autograd.grad may cause an error, make sure the grads is a PyTorhc Tensor variable\n",
    "print(grads.shape)\n",
    "\n",
    "# calculate the pertubation using the sign of grads\n",
    "\n",
    "pertubation = ??? grads ???\n",
    "\n",
    "# apply pertubation, \n",
    "\n",
    "x_adv = \n",
    "\n",
    "# now that we have the adversarial examples\n",
    "# get the prediction result and print the adversarial example\n",
    "\n",
    "\n",
    "print( 'our adversarial example' )\n",
    "print( x_adv.shape )\n",
    "\n",
    "print( 'logits for our sample: \\t\\n', ???  )\n",
    "print( 'class prediction for our sample: \\t\\n',  ???  )\n",
    "print( 'predicted as',  ???  )\n",
    "\n",
    "plt.imshow( x_adv.detach().numpy().reshape( 28, 28 ), cmap=\"gray_r\" )\n",
    "plt.axis( 'off' )\n",
    "plt.show( )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "azepEwpTewML"
   },
   "source": [
    "The FGSM is one of the most simple attacks.\n",
    "As we can see that results are not very convincing since the perturbation is perceptible.\n",
    "We can improve on it by making it iterative. \n",
    "\n",
    "Using the code from above, create an iterative version of FGSM that calculates a new perturbation with a smaller epsilon for every iteration and stops once it achieves mis-classification.\n",
    "\n",
    "The goal is to make the perturbation as small/invisible as possible, but also make the model give wrong prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mOHo8eQ0gS4F"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "epsilon =\n",
    "iterations =\n",
    "for i in range(iterations):\n",
    "    # your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VszDZ1p6V7Hc"
   },
   "source": [
    "Let's use `ART` library to do the actual attack magic.\n",
    "\n",
    "we will also use the FGSM attack to generate an adversarial example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YrbyOs_3V7He"
   },
   "outputs": [],
   "source": [
    "# using the ART implemenation\n",
    "\n",
    "\n",
    "print( 'logits for our sample: \\t\\n', ???  )\n",
    "print( 'class prediction for our sample: \\t\\n',  ???  )\n",
    "print( 'predicted as',  ???  )\n",
    "\n",
    "# visualize it\n",
    "\n",
    "plt.imshow( adv_sample.reshape( 28, 28 ), cmap=\"gray_r\" )\n",
    "plt.axis( 'off' )\n",
    "plt.show( )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that it's much simpler than we write it from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check it from https://github.com/Trusted-AI/adversarial-robustness-toolbox/blob/main/examples/get_started_pytorch.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vKJQIF24Y91W"
   },
   "source": [
    "We have seen that FGSM does not do a great job of producing adversarial examples when work with 0 and 1. Update the code above work on all 10 digits and try for a number of 0 instance what class they get transformed into in an untargeted attack.\n",
    "Alternativley pick a pair of numbers that you think are closer to each orther and the FGSM attack should work better with.\n",
    "\n",
    "\n",
    "`ART` provides more attacks than the once introdcued above. Try any other attacks from the official documents.\n",
    "\n",
    "You can find more information on the attacks here: https://github.com/Trusted-AI/adversarial-robustness-toolbox/wiki/ART-Attacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uvZ3hfrNcp9i"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "attack_cnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}